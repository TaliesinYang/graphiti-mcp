#!/usr/bin/env python3
"""
æ™ºèƒ½åŒé‡æ£€ç´¢ç³»ç»Ÿ - MCPé›†æˆç‰ˆæœ¬
Phase 3: MCP Integration
"""

import json
import os
import logging
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
import asyncio
from graphiti_core import Graphiti
from graphiti_core.edges import EntityEdge

# é…ç½®logger
logger = logging.getLogger(__name__)


class SmartRetrievalMCP:
    """æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿï¼šMCPé›†æˆç‰ˆæœ¬"""
    
    def __init__(self, graphiti_client: Optional[Graphiti] = None):
        self.graphiti_client = graphiti_client
        self.search_history = []
        self.term_associations = {}
        self.learning_enabled = True
        self.max_facts = 20
        self.max_episodes = 50
        
        # åŠ è½½å·²æœ‰çš„å­¦ä¹ æ•°æ®
        self.load_learning_data()
    
    async def search(self, 
                    query: str, 
                    group_ids: Optional[List[str]] = None,
                    learn: bool = True) -> Dict[str, Any]:
        """
        ä¸»æ£€ç´¢æ–¹æ³• - Factså‘ç° + Episodesè·å–
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            group_ids: é™å®šçš„group_idåˆ—è¡¨
            learn: æ˜¯å¦å¯ç”¨å­¦ä¹ 
            
        Returns:
            åŒ…å«factsã€episodeså’Œå­¦ä¹ ä¿¡æ¯çš„ç»“æœå­—å…¸
        """
        start_time = datetime.now()
        
        # é»˜è®¤group_ids
        if not group_ids:
            group_ids = ["volunteer-job-procedures", "volunteer-job-preferences", "volunteer-job"]
        
        # Step 1: æŸ¥è¯¢æ‰©å±•ï¼ˆä½¿ç”¨å·²å­¦ä¹ çš„å…³è”ï¼‰
        expanded_query = self._expand_query(query)
        
        # Step 2: Factså¿«é€Ÿå‘ç°
        facts = await self._search_facts(expanded_query, group_ids)
        
        # Step 3: ä»Factsæå–Episode UUIDs
        episode_uuids = self._extract_episode_uuids(facts)
        
        # Step 4: è·å–å®Œæ•´Episodes
        episodes = await self._fetch_episodes(episode_uuids, group_ids)
        
        # Step 5: å­¦ä¹ å…³è”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if learn and self.learning_enabled:
            self._learn_from_results(query, facts, episodes)
        
        # Step 6: æ„å»ºç»“æœ
        result = {
            'query': query,
            'expanded_query': expanded_query,
            'facts': facts,
            'episodes': episodes,
            'facts_count': len(facts),
            'episodes_count': len(episodes),
            'search_time': (datetime.now() - start_time).total_seconds(),
            'learned_terms': self.term_associations.get(query, [])
        }
        
        # è®°å½•æœç´¢å†å²
        self.search_history.append({
            'timestamp': datetime.now().isoformat(),
            'query': query,
            'results': result
        })
        
        return result
    
    def _expand_query(self, query: str) -> str:
        """ä½¿ç”¨å­¦ä¹ çš„å…³è”æ‰©å±•æŸ¥è¯¢"""
        expanded_terms = [query]
        
        # æ·»åŠ ç›´æ¥å…³è”çš„è¯
        if query in self.term_associations:
            expanded_terms.extend(self.term_associations[query][:5])
        
        # æ£€æŸ¥æŸ¥è¯¢ä¸­çš„æ¯ä¸ªè¯
        words = query.split()
        for word in words:
            if word in self.term_associations:
                expanded_terms.extend(self.term_associations[word][:3])
        
        # å»é‡å¹¶ç»„åˆ
        unique_terms = []
        seen = set()
        for term in expanded_terms:
            if term.lower() not in seen:
                seen.add(term.lower())
                unique_terms.append(term)
        
        return ' '.join(unique_terms)
    
    async def _search_facts(self, query: str, group_ids: List[str]) -> List[Dict]:
        """æ‰§è¡ŒFactsæœç´¢"""
        if not self.graphiti_client:
            return []
        
        try:
            # ä½¿ç”¨Graphitiå®¢æˆ·ç«¯æœç´¢
            search_results = await self.graphiti_client.search(
                query=query,
                num_results=self.max_facts,
                group_ids=group_ids
            )
            
            # æ ¼å¼åŒ–Factsç»“æœ
            facts = []
            for edge in search_results:
                fact_dict = {
                    'uuid': edge.uuid,
                    'fact': edge.fact,
                    'source_node_uuid': edge.source_node_uuid,
                    'target_node_uuid': edge.target_node_uuid,
                    'episodes': edge.episodes if hasattr(edge, 'episodes') else [],
                    'created_at': edge.created_at.isoformat() if edge.created_at else None
                }
                facts.append(fact_dict)
            
            return facts
        except Exception as e:
            print(f"âš ï¸ Error searching facts: {e}")
            return []
    
    def _extract_episode_uuids(self, facts: List[Dict]) -> List[str]:
        """ä»Factsä¸­æå–Episode UUIDs"""
        episode_uuids = set()
        for fact in facts:
            episodes = fact.get('episodes', [])
            if isinstance(episodes, list):
                episode_uuids.update(episodes)
        return list(episode_uuids)
    
    async def _fetch_episodes(self, uuids: List[str], group_ids: List[str]) -> List[Dict]:
        """æ‰¹é‡è·å–Episodes"""
        if not self.graphiti_client or not uuids:
            return []
        
        episodes = []
        try:
            # è·å–æ‰€æœ‰groupçš„episodes
            for group_id in group_ids:
                group_episodes = await self.graphiti_client.driver.get_episodes(
                    group_id=group_id,
                    last_n=self.max_episodes
                )
                
                # è¿‡æ»¤å‡ºåŒ¹é…UUIDçš„episodes
                for episode in group_episodes:
                    if hasattr(episode, 'uuid') and episode.uuid in uuids:
                        episode_dict = {
                            'uuid': episode.uuid,
                            'name': episode.name if hasattr(episode, 'name') else '',
                            'content': episode.content if hasattr(episode, 'content') else '',
                            'group_id': episode.group_id if hasattr(episode, 'group_id') else '',
                            'created_at': episode.created_at.isoformat() if hasattr(episode, 'created_at') else None,
                            'source': episode.source if hasattr(episode, 'source') else 'text'
                        }
                        episodes.append(episode_dict)
                        
        except Exception as e:
            print(f"âš ï¸ Error fetching episodes: {e}")
        
        return episodes
    
    async def get_episode_by_uuid(self, uuid: str) -> Optional[Dict]:
        """æŒ‰UUIDç›´æ¥æŸ¥è¯¢Episode - ä¼˜åŒ–ç‰ˆæœ¬"""
        if not self.graphiti_client or not self.graphiti_client.driver:
            logger.error("Graphiti client or driver not initialized")
            return None
        
        try:
            # ç›´æ¥ä½¿ç”¨CypheræŸ¥è¯¢ï¼Œåˆ©ç”¨Neo4jç´¢å¼•
            query = """
            MATCH (e:Episodic {uuid: $uuid})
            RETURN
                e.uuid AS uuid,
                e.name AS name,
                e.content AS content,
                e.group_id AS group_id,
                e.created_at AS created_at,
                e.valid_at AS valid_at,
                e.source AS source,
                e.source_description AS source_description,
                e.entity_edges AS entity_edges
            LIMIT 1
            """
            
            # æ‰§è¡ŒæŸ¥è¯¢
            records, _, _ = await self.graphiti_client.driver.execute_query(
                query,
                uuid=uuid,
                routing_='r'  # åªè¯»è·¯ç”±
            )
            
            # å¦‚æœæ‰¾åˆ°äº†è®°å½•
            if records and len(records) > 0:
                record = records[0]
                return {
                    'uuid': record.get('uuid'),
                    'name': record.get('name', ''),
                    'content': record.get('content', ''),
                    'group_id': record.get('group_id', ''),
                    'created_at': record.get('created_at').isoformat() if record.get('created_at') else None,
                    'valid_at': record.get('valid_at').isoformat() if record.get('valid_at') else None,
                    'source': record.get('source', 'text'),
                    'source_description': record.get('source_description', ''),
                    'entity_edges': record.get('entity_edges', [])
                }
            
            logger.debug(f"Episode with UUID {uuid} not found")
            return None
            
        except Exception as e:
            logger.error(f"Error getting episode by UUID {uuid}: {str(e)}")
            return None
    
    def _learn_from_results(self, query: str, facts: List[Dict], episodes: List[Dict]):
        """ä»æœç´¢ç»“æœä¸­å­¦ä¹ å…³è”"""
        learned_terms = set()
        
        # ä»Factsä¸­å­¦ä¹ 
        for fact in facts[:10]:
            fact_text = fact.get('fact', '')
            words = fact_text.split()
            for word in words:
                if len(word) > 2:
                    learned_terms.add(word.lower())
        
        # ä»Episodesä¸­å­¦ä¹ å…³é”®è¯
        for episode in episodes[:5]:
            content = episode.get('content', '')
            if isinstance(content, dict):
                # æå–JSONä¸­çš„keywords
                keywords = content.get('keywords', {})
                if isinstance(keywords, dict):
                    for key_type in ['chinese', 'english', 'technical']:
                        terms = keywords.get(key_type, [])
                        if isinstance(terms, list):
                            learned_terms.update([t.lower() for t in terms if isinstance(t, str)])
        
        # æ›´æ–°å…³è”è®°å¿†
        if query not in self.term_associations:
            self.term_associations[query] = []
        
        existing = set(self.term_associations[query])
        new_terms = learned_terms - existing - {query.lower()}
        
        self.term_associations[query].extend(list(new_terms)[:10])
        
        # ä¿å­˜å­¦ä¹ æ•°æ®
        self.save_learning_data()
        
        print(f"ğŸ§  Learned {len(new_terms)} new associations for '{query}'")
    
    def save_learning_data(self):
        """ä¿å­˜å­¦ä¹ æ•°æ®åˆ°æœ¬åœ°æ–‡ä»¶"""
        data = {
            'term_associations': self.term_associations,
            'last_updated': datetime.now().isoformat()
        }
        
        file_path = os.path.join(
            os.path.dirname(__file__),
            'search_learning.json'
        )
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ Failed to save learning data: {e}")
    
    def load_learning_data(self):
        """åŠ è½½å·²æœ‰çš„å­¦ä¹ æ•°æ®"""
        file_path = os.path.join(
            os.path.dirname(__file__),
            'search_learning.json'
        )
        
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.term_associations = data.get('term_associations', {})
                    print(f"ğŸ“– Loaded {len(self.term_associations)} learned associations")
            except Exception as e:
                print(f"âš ï¸ Failed to load learning data: {e}")
    
    def get_search_stats(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_searches': len(self.search_history),
            'learned_queries': len(self.term_associations),
            'total_associations': sum(len(v) for v in self.term_associations.values()),
            'recent_queries': [s['query'] for s in self.search_history[-5:]],
            'top_learned': sorted(
                self.term_associations.items(), 
                key=lambda x: len(x[1]), 
                reverse=True
            )[:5]
        }